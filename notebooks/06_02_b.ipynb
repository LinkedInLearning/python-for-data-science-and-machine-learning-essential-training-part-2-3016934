{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to NLP"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "import nltk\n",
                "# Ensure common nltk resources are available quietly\n",
                "for pkg in ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']:\n",
                "    try:\n",
                "        nltk.data.find(pkg)\n",
                "    except Exception:\n",
                "        nltk.download(pkg, quiet=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: nltk in /workspaces/python-for-data-science-and-machine-learning-essential-training-part-2-3016934/.venv/lib/python3.12/site-packages (3.9.2)\n",
                        "Requirement already satisfied: click in /workspaces/python-for-data-science-and-machine-learning-essential-training-part-2-3016934/.venv/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
                        "Requirement already satisfied: joblib in /workspaces/python-for-data-science-and-machine-learning-essential-training-part-2-3016934/.venv/lib/python3.12/site-packages (from nltk) (1.5.3)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /workspaces/python-for-data-science-and-machine-learning-essential-training-part-2-3016934/.venv/lib/python3.12/site-packages (from nltk) (2026.1.15)\n",
                        "Requirement already satisfied: tqdm in /workspaces/python-for-data-science-and-machine-learning-essential-training-part-2-3016934/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
                    ]
                }
            ],
            "source": [
                "!pip install nltk\n",
                "import nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"On Wednesday, the Association for Computing Machinery, the world\u2019s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year\u2019s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nltk.download('punkt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'text' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sent_tk = sent_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSentence tokenizing the text: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(sent_tk)\n",
                        "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import sent_tokenize\n",
                "sent_tk = sent_tokenize(text)\n",
                "print(\"Sentence tokenizing the text: \\n\")\n",
                "print(sent_tk)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'text' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m word_tk = word_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWord tokenizing the text: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(word_tk)\n",
                        "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import word_tokenize\n",
                "word_tk = word_tokenize(text)\n",
                "print(\"Word tokenizing the text: \\n\")\n",
                "print(word_tk)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     /home/codespace/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stop words in English language are: \n",
                        "\n",
                        "{'why', 'mightn', 'am', \"doesn't\", 'if', 'it', \"he's\", 'between', 'off', 'only', \"hasn't\", \"we'll\", 'that', \"it's\", 'more', \"won't\", 'below', 'as', 'hers', 'all', 'just', 'didn', \"we've\", 'at', 'nor', 'mustn', 'ma', \"you'd\", 'isn', 'her', \"he'll\", 'not', 'those', 'from', 'against', \"i'll\", \"they'd\", 'myself', \"mustn't\", \"hadn't\", 'most', 'while', 'won', 'again', \"it'd\", \"that'll\", \"we're\", 'such', 'to', \"i'm\", 'these', 'how', 'can', \"weren't\", 'its', 'wouldn', 's', 'once', 'his', 'so', \"she's\", 'under', 'there', 've', 'shouldn', 'has', \"you'll\", 'he', 'same', 'ain', \"i'd\", 'don', \"shan't\", 'before', 'she', 'does', 'then', 'wasn', 'yourself', 'no', 'with', \"they've\", \"mightn't\", 'because', 'who', 'yours', 'doesn', 'yourselves', 'have', \"wouldn't\", 'when', \"i've\", 'up', 'y', 'an', 'himself', \"didn't\", \"they'll\", 'down', 'him', 'their', 'what', \"you're\", 'is', 'were', 'had', 're', 'i', 'both', 'on', 'needn', 'your', 'until', 'm', \"he'd\", 'doing', 'this', 'some', 'very', 'we', \"haven't\", \"needn't\", \"they're\", 'in', \"couldn't\", 'of', \"should've\", \"you've\", 'you', \"aren't\", 'above', 'been', 'do', 'for', 'over', 'shan', \"wasn't\", 'o', 'too', \"shouldn't\", 'hadn', 'few', 'further', 'own', 'did', 'haven', 'ourselves', 'than', \"isn't\", 'whom', 'by', 'into', 'my', 'd', 'ours', \"we'd\", 'itself', 'where', 'herself', 'after', 'now', 'themselves', 'are', 't', 'and', 'our', 'during', 'weren', 'was', 'will', 'aren', \"don't\", 'they', 'each', 'having', \"it'll\", 'here', 'or', 'but', 'other', 'a', 'couldn', 'hasn', 'out', \"she'd\", 'be', 'about', 'any', 'through', 'theirs', 'being', 'll', 'the', \"she'll\", 'which', 'them', 'should', 'me'}\n"
                    ]
                }
            ],
            "source": [
                "from nltk.corpus import stopwords\n",
                "\n",
                "sw = set(stopwords.words(\"english\"))\n",
                "print(\"Stop words in English language are: \\n\")\n",
                "print(sw)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'word_tk' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m filtered_words = [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tk\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sw]\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe text after removing stop words \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(filtered_words)\n",
                        "\u001b[31mNameError\u001b[39m: name 'word_tk' is not defined"
                    ]
                }
            ],
            "source": [
                "filtered_words = [w for w in word_tk if not w in sw]\n",
                "\n",
                "print(\"The text after removing stop words \\n\")\n",
                "print(filtered_words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "\n",
                "port_stem = PorterStemmer()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'filtered_words' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m stemmed_words = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfiltered_words\u001b[49m:\n\u001b[32m      4\u001b[39m     stemmed_words.append(port_stem.stem(w))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFiltered Sentence: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, filtered_words, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'filtered_words' is not defined"
                    ]
                }
            ],
            "source": [
                "stemmed_words = []\n",
                "\n",
                "for w in filtered_words:\n",
                "    stemmed_words.append(port_stem.stem(w))\n",
                "    \n",
                "print(\"Filtered Sentence: \\n\", filtered_words, \"\\n\")\n",
                "print(\"Stemmed Sentence: \\n\", stemmed_words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}